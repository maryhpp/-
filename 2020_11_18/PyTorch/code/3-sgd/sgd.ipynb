{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 随机梯度下降法\n",
    "前面我们介绍了梯度下降法的数学原理，下面我们通过例子来说明一下随机梯度下降法。我们先自己实现，然后使用 pytorch 中自带的优化器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 实现SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "随机梯度下降法非常简单，公式就是\n",
    "$$\n",
    "\\theta_{i+1} = \\theta_i - \\eta \\nabla L(\\theta)\n",
    "$$\n",
    "非常简单，我们可以从 0 开始自己实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def sgd_update(parameters, lr):\n",
    "    for param in parameters:\n",
    "        param.data = param.data - lr * param.grad.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torchvision.datasets import MNIST # 导入 pytorch 内置的 mnist 数据\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def data(x):\n",
    "    x = np.array(x, dtype='float32') / 255 # 将数据变到 0 ~ 1 之间\n",
    "    x = (x - 0.5) / 0.5 # 标准化，这个技巧之后会讲到\n",
    "    x = x.reshape((-1,)) # 拉平\n",
    "    x = torch.from_numpy(x)\n",
    "    return x\n",
    "\n",
    "def train(batch_size,lr):\n",
    "    train_data = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "    # 使用 Sequential 定义 3 层神经网络\n",
    "    net = nn.Sequential(\n",
    "        nn.Linear(784, 200),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(200, 10),\n",
    "    )\n",
    "    #net=net.cuda()\n",
    "    # 开始训练\n",
    "    losses = []\n",
    "    idx = 0\n",
    "\n",
    "    start = time.time() # 记时开始\n",
    "    for e in range(1):\n",
    "        train_loss = 0\n",
    "        for im, label in train_data:\n",
    "            im = Variable(im)#.cuda()\n",
    "            label = Variable(label)#.cuda()\n",
    "            # 前向传播\n",
    "            out = net(im)\n",
    "            loss = criterion(out, label)\n",
    "            # 反向传播\n",
    "            net.zero_grad()\n",
    "            loss.backward()\n",
    "            sgd_update(net.parameters(),lr) # 使用lr的学习率\n",
    "            # 记录误差\n",
    "            train_loss += loss.item()\n",
    "            if idx % 30 == 0:\n",
    "                losses.append(loss.item())\n",
    "            idx += 1\n",
    "        print('epoch: {}, Train Loss: {:.6f}'.format(e, train_loss / len(train_data)))\n",
    "    end = time.time() # 计时结束\n",
    "    print('使用时间: {:.5f} s'.format(end - start))\n",
    "    x_axis = np.linspace(0, 5, len(losses), endpoint=True)\n",
    "    plt.semilogy(x_axis, losses, label='batch_size=%d'%batch_size)\n",
    "    plt.legend(loc='best')\n",
    "\n",
    "train_set = MNIST('./data', train=True, transform=data, download=True) # 载入数据集，申明定义的数据变换\n",
    "test_set = MNIST('./data', train=False, transform=data, download=True)\n",
    "\n",
    "# 定义 loss 函数\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以将 batch size 先设置为 1，看看训练效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train( batch_size=1,lr=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，loss 在剧烈震荡，因为每次都是只对一个样本点做计算，每一层的梯度都具有很高的随机性，而且需要耗费了大量的时间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train( batch_size=64,lr=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过上面的结果可以看到 loss 没有 batchsize=1时震荡那么剧烈，同时loss值也降到一定程度了，梯度相对而言更稳定。不过batch size 太大，对于内存的需求就更高，同时也不利于网络跳出局部极小点，所以现在普遍使用基于 batch 的随机梯度下降法，而 batch 的取值根据实际情况选择。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们调高学习率，看看训练效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train( batch_size=64,lr=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，学习率太大会使得损失函数不断回跳，从而无法让损失函数较好降低，所以我们一般都是用一个比较小的学习率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 使用Pytorch内置的SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pytorch 中已经为我们内置了随机梯度下降发，而且之前我们一直在使用，下面我们来使用 pytorch 自带的优化器来实现随机梯度下降"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data = DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "# 使用 Sequential 定义 3 层神经网络\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(784, 200),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(200, 10),\n",
    ")\n",
    "net=net#.cuda()\n",
    "optimzier = torch.optim.SGD(net.parameters(), 1e-2)\n",
    "\n",
    "# 开始训练\n",
    "start = time.time() # 记时开始\n",
    "for e in range(1):\n",
    "    train_loss = 0\n",
    "    for im, label in train_data:\n",
    "        im = Variable(im)#.cuda()\n",
    "        label = Variable(label)#.cuda()\n",
    "        # 前向传播\n",
    "        out = net(im)\n",
    "        loss = criterion(out, label)\n",
    "        # 反向传播\n",
    "        optimzier.zero_grad()\n",
    "        loss.backward()\n",
    "        optimzier.step()\n",
    "        # 记录误差\n",
    "        train_loss += loss.item()\n",
    "    print('epoch: {}, Train Loss: {:.6f}'\n",
    "          .format(e, train_loss / len(train_data)))\n",
    "end = time.time() # 计时结束\n",
    "print('使用时间: {:.5f} s'.format(end - start))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
